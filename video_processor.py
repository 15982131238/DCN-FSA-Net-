#!/usr/bin/env python3
"""
è§†é¢‘å¤„ç†æ¨¡å—
æ”¯æŒå®æ—¶è§†é¢‘æµå’Œæ–‡ä»¶è§†é¢‘å¤„ç†
"""

import cv2
import numpy as np
import torch
import time
from PIL import Image
from typing import List, Dict, Any, Optional, Callable
import threading
import queue
import logging

logger = logging.getLogger(__name__)

class VideoProcessor:
    """è§†é¢‘å¤„ç†å™¨"""

    def __init__(self, model, device, recognition_callback: Optional[Callable] = None):
        self.model = model
        self.device = device
        self.recognition_callback = recognition_callback
        self.is_running = False
        self.frame_queue = queue.Queue(maxsize=10)
        self.result_queue = queue.Queue(maxsize=10)
        self.processing_thread = None
        self.capture_thread = None

    def process_frame(self, frame: np.ndarray) -> Dict[str, Any]:
        """å¤„ç†å•å¸§å›¾åƒ"""
        try:
            # è½¬æ¢BGRåˆ°RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # è½¬æ¢ä¸ºPILå›¾åƒ
            image = Image.fromarray(frame_rgb)

            # è°ƒç”¨è¯†åˆ«å‡½æ•°
            from plate_recognition_api import recognize_plate
            result = recognize_plate(image)

            return result

        except Exception as e:
            logger.error(f"å¸§å¤„ç†å¤±è´¥: {e}")
            return {
                "plate_number": "å¤„ç†å¤±è´¥",
                "plate_type": "æœªçŸ¥",
                "confidence": 0.0,
                "processing_time": 0.0,
                "error": str(e)
            }

    def draw_result(self, frame: np.ndarray, result: Dict[str, Any]) -> np.ndarray:
        """åœ¨å¸§ä¸Šç»˜åˆ¶è¯†åˆ«ç»“æœ"""
        try:
            # å¤åˆ¶å¸§
            output_frame = frame.copy()

            # è·å–å¸§å°ºå¯¸
            height, width = frame.shape[:2]

            # ç»˜åˆ¶åŠé€æ˜èƒŒæ™¯
            overlay = output_frame.copy()
            cv2.rectangle(overlay, (0, height-100), (width, height), (0, 0, 0), -1)
            cv2.addWeighted(overlay, 0.7, output_frame, 0.3, 0, output_frame)

            # ç»˜åˆ¶è¯†åˆ«ç»“æœ
            plate_text = f"è½¦ç‰Œ: {result.get('plate_number', 'N/A')}"
            type_text = f"ç±»å‹: {result.get('plate_type', 'N/A')}"
            confidence_text = f"ç½®ä¿¡åº¦: {result.get('confidence', 0)*100:.1f}%"

            # è®¾ç½®å­—ä½“
            font = cv2.FONT_HERSHEY_SIMPLEX
            font_scale = 0.7
            font_thickness = 2

            # è®¡ç®—æ–‡æœ¬ä½ç½®
            y_offset = height - 70

            # ç»˜åˆ¶æ–‡æœ¬
            cv2.putText(output_frame, plate_text, (10, y_offset),
                       font, font_scale, (255, 255, 255), font_thickness)
            cv2.putText(output_frame, type_text, (10, y_offset + 25),
                       font, font_scale, (255, 255, 255), font_thickness)
            cv2.putText(output_frame, confidence_text, (10, y_offset + 50),
                       font, font_scale, (255, 255, 255), font_thickness)

            # ç»˜åˆ¶ç½®ä¿¡åº¦æ¡
            if 'confidence' in result:
                confidence = result['confidence']
                bar_width = int((width - 20) * confidence)
                cv2.rectangle(output_frame, (10, y_offset + 65),
                             (10 + bar_width, y_offset + 75), (0, 255, 0), -1)
                cv2.rectangle(output_frame, (10, y_offset + 65),
                             (width - 10, y_offset + 75), (255, 255, 255), 2)

            return output_frame

        except Exception as e:
            logger.error(f"ç»˜åˆ¶ç»“æœå¤±è´¥: {e}")
            return frame

    def capture_frames(self, source, fps_limit=10):
        """æ•è·å¸§çº¿ç¨‹"""
        cap = cv2.VideoCapture(source)

        if not cap.isOpened():
            logger.error("æ— æ³•æ‰“å¼€è§†é¢‘æº")
            return

        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        logger.info(f"è§†é¢‘æºä¿¡æ¯: {width}x{height}, {fps:.2f}fps")

        # è®¡ç®—å¸§é—´éš”
        frame_interval = 1.0 / fps_limit if fps_limit > 0 else 0
        last_time = time.time()

        while self.is_running:
            ret, frame = cap.read()

            if not ret:
                logger.info("è§†é¢‘ç»“æŸæˆ–è¯»å–å¤±è´¥")
                break

            # æ§åˆ¶å¸§ç‡
            current_time = time.time()
            if current_time - last_time < frame_interval:
                time.sleep(0.001)
                continue

            last_time = current_time

            # å°†å¸§æ”¾å…¥é˜Ÿåˆ—
            try:
                self.frame_queue.put(frame, timeout=0.1)
            except queue.Full:
                # é˜Ÿåˆ—æ»¡ï¼Œä¸¢å¼ƒæœ€æ—§çš„å¸§
                try:
                    self.frame_queue.get_nowait()
                    self.frame_queue.put(frame, timeout=0.1)
                except queue.Empty:
                    pass

        cap.release()
        logger.info("å¸§æ•è·çº¿ç¨‹ç»“æŸ")

    def process_frames(self):
        """å¤„ç†å¸§çº¿ç¨‹"""
        while self.is_running:
            try:
                # ä»é˜Ÿåˆ—è·å–å¸§
                frame = self.frame_queue.get(timeout=1.0)

                # å¤„ç†å¸§
                result = self.process_frame(frame)

                # ç»˜åˆ¶ç»“æœ
                output_frame = self.draw_result(frame, result)

                # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—
                self.result_queue.put((output_frame, result), timeout=0.1)

                # è°ƒç”¨å›è°ƒå‡½æ•°
                if self.recognition_callback:
                    self.recognition_callback(result)

            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"å¸§å¤„ç†é”™è¯¯: {e}")

        logger.info("å¸§å¤„ç†çº¿ç¨‹ç»“æŸ")

    def start(self, source, fps_limit=10):
        """å¯åŠ¨è§†é¢‘å¤„ç†"""
        if self.is_running:
            logger.warning("è§†é¢‘å¤„ç†å™¨å·²åœ¨è¿è¡Œ")
            return False

        self.is_running = True

        # å¯åŠ¨æ•è·çº¿ç¨‹
        self.capture_thread = threading.Thread(
            target=self.capture_frames,
            args=(source, fps_limit)
        )
        self.capture_thread.daemon = True
        self.capture_thread.start()

        # å¯åŠ¨å¤„ç†çº¿ç¨‹
        self.processing_thread = threading.Thread(target=self.process_frames)
        self.processing_thread.daemon = True
        self.processing_thread.start()

        logger.info("è§†é¢‘å¤„ç†å™¨å¯åŠ¨æˆåŠŸ")
        return True

    def stop(self):
        """åœæ­¢è§†é¢‘å¤„ç†"""
        self.is_running = False

        if self.capture_thread:
            self.capture_thread.join(timeout=1.0)

        if self.processing_thread:
            self.processing_thread.join(timeout=1.0)

        # æ¸…ç©ºé˜Ÿåˆ—
        while not self.frame_queue.empty():
            try:
                self.frame_queue.get_nowait()
            except queue.Empty:
                break

        while not self.result_queue.empty():
            try:
                self.result_queue.get_nowait()
            except queue.Empty:
                break

        logger.info("è§†é¢‘å¤„ç†å™¨å·²åœæ­¢")

    def get_result(self):
        """è·å–å¤„ç†ç»“æœ"""
        try:
            return self.result_queue.get(timeout=1.0)
        except queue.Empty:
            return None, None

class VideoStreamer:
    """è§†é¢‘æµå¤„ç†å™¨"""

    def __init__(self, model, device):
        self.model = model
        self.device = device
        self.processor = None
        self.window_name = "è½¦ç‰Œè¯†åˆ«å®æ—¶è§†é¢‘"

    def start_camera(self, camera_id=0, fps_limit=10):
        """å¯åŠ¨æ‘„åƒå¤´"""
        self.processor = VideoProcessor(
            self.model,
            self.device,
            self.on_recognition_result
        )

        # åˆ›å»ºçª—å£
        cv2.namedWindow(self.window_name, cv2.WINDOW_NORMAL)
        cv2.resizeWindow(self.window_name, 800, 600)

        # å¯åŠ¨å¤„ç†å™¨
        if self.processor.start(camera_id, fps_limit):
            print("ğŸ“¹ æ‘„åƒå¤´å¯åŠ¨æˆåŠŸ")
            print("æŒ‰ ESC é”®é€€å‡º")

            # æ˜¾ç¤ºå¾ªç¯
            while True:
                frame, result = self.processor.get_result()

                if frame is not None:
                    cv2.imshow(self.window_name, frame)

                # æ£€æŸ¥æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == 27:  # ESCé”®
                    break

            # åœæ­¢å¤„ç†å™¨
            self.processor.stop()
            cv2.destroyAllWindows()
            print("ğŸ›‘ æ‘„åƒå¤´å·²åœæ­¢")
        else:
            print("âŒ æ‘„åƒå¤´å¯åŠ¨å¤±è´¥")

    def process_video_file(self, video_path, output_path=None, fps_limit=10):
        """å¤„ç†è§†é¢‘æ–‡ä»¶"""
        if not os.path.exists(video_path):
            print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return False

        self.processor = VideoProcessor(
            self.model,
            self.device,
            self.on_recognition_result
        )

        # åˆ›å»ºçª—å£
        cv2.namedWindow(self.window_name, cv2.WINDOW_NORMAL)
        cv2.resizeWindow(self.window_name, 800, 600)

        # è®¾ç½®è§†é¢‘å†™å…¥å™¨
        video_writer = None
        if output_path:
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            cap.release()

            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            video_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        # å¯åŠ¨å¤„ç†å™¨
        if self.processor.start(video_path, fps_limit):
            print(f"ğŸ“¹ å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")
            print("æŒ‰ ESC é”®é€€å‡º")

            # æ˜¾ç¤ºå¾ªç¯
            while True:
                frame, result = self.processor.get_result()

                if frame is not None:
                    cv2.imshow(self.window_name, frame)

                    # å†™å…¥è¾“å‡ºæ–‡ä»¶
                    if video_writer:
                        video_writer.write(frame)

                # æ£€æŸ¥æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == 27:  # ESCé”®
                    break

                # æ£€æŸ¥æ˜¯å¦å¤„ç†å®Œæˆ
                if not self.processor.is_running and self.processor.frame_queue.empty():
                    break

            # åœæ­¢å¤„ç†å™¨
            self.processor.stop()
            if video_writer:
                video_writer.release()
            cv2.destroyAllWindows()
            print("ğŸ›‘ è§†é¢‘å¤„ç†å®Œæˆ")

            if output_path:
                print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")

            return True
        else:
            print("âŒ è§†é¢‘å¤„ç†å¤±è´¥")
            return False

    def on_recognition_result(self, result):
        """è¯†åˆ«ç»“æœå›è°ƒ"""
        if result.get('plate_number') != 'å¤„ç†å¤±è´¥':
            print(f"ğŸš— è¯†åˆ«ç»“æœ: {result.get('plate_number')} "
                  f"({result.get('plate_type')}) "
                  f"ç½®ä¿¡åº¦: {result.get('confidence', 0)*100:.1f}%")

def create_video_demo():
    """åˆ›å»ºè§†é¢‘æ¼”ç¤ºè„šæœ¬"""
    script_content = '''#!/usr/bin/env python3
"""
è§†é¢‘å¤„ç†æ¼”ç¤º
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from video_processor import VideoStreamer
from plate_recognition_api import load_model, model

def main():
    # åŠ è½½æ¨¡å‹
    if not load_model():
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
        return

    # åˆ›å»ºè§†é¢‘æµå¤„ç†å™¨
    streamer = VideoStreamer(model, model.device)

    print("ğŸš— è½¦ç‰Œè¯†åˆ«è§†é¢‘å¤„ç†æ¼”ç¤º")
    print("1. æ‘„åƒå¤´å®æ—¶è¯†åˆ«")
    print("2. è§†é¢‘æ–‡ä»¶å¤„ç†")
    print("3. é€€å‡º")

    while True:
        choice = input("è¯·é€‰æ‹©åŠŸèƒ½ (1-3): ").strip()

        if choice == '1':
            camera_id = input("è¯·è¾“å…¥æ‘„åƒå¤´ID (é»˜è®¤0): ").strip()
            camera_id = int(camera_id) if camera_id.isdigit() else 0
            streamer.start_camera(camera_id)

        elif choice == '2':
            video_path = input("è¯·è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„: ").strip()
            if os.path.exists(video_path):
                output_path = input("è¯·è¾“å…¥è¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¯é€‰): ").strip()
                output_path = output_path if output_path else None
                streamer.process_video_file(video_path, output_path)
            else:
                print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")

        elif choice == '3':
            print("ğŸ‘‹ å†è§!")
            break

        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main()
'''

    with open("video_demo.py", "w", encoding="utf-8") as f:
        f.write(script_content)

    print("è§†é¢‘æ¼”ç¤ºè„šæœ¬å·²åˆ›å»º: video_demo.py")

if __name__ == "__main__":
    # åˆ›å»ºè§†é¢‘æ¼”ç¤ºè„šæœ¬
    create_video_demo()