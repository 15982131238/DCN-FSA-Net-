#!/usr/bin/env python3
"""
ç»ˆæOCRè½¦ç‰Œè¯†åˆ«ç³»ç»Ÿ - çœŸå®æ–‡å­—æå– + è®­ç»ƒç»“æœä¼˜åŒ–
ç»“åˆçœŸå®OCRæŠ€æœ¯ä¸è®­ç»ƒæ¨¡å‹ï¼Œç¡®ä¿è¯†åˆ«ç»“æœä¸åŸå§‹å›¾ç‰‡å®Œå…¨ä¸€è‡´
"""

import os
import sys
import logging
import time
import json
import sqlite3
import re
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from datetime import datetime
import io
import base64

import numpy as np
from PIL import Image, ImageDraw, ImageFont, ImageEnhance, ImageFilter, ImageOps
import cv2
import torch
import torch.nn as nn
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import uvicorn
from starlette.middleware.cors import CORSMiddleware

# å°è¯•å¯¼å…¥OCRåº“
try:
    import pytesseract
    TESSERACT_AVAILABLE = True
    print("Tesseract OCRå·²åŠ è½½")
except ImportError:
    TESSERACT_AVAILABLE = False
    print("Tesseract OCRä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨OpenCVè¿›è¡Œæ–‡å­—æ£€æµ‹")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–FastAPIåº”ç”¨
app = FastAPI(title="ç»ˆæOCRè½¦ç‰Œè¯†åˆ«ç³»ç»Ÿ", version="6.0.0")

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®é™æ€æ–‡ä»¶
app.mount("/static", StaticFiles(directory="static"), name="static")

# è½¦ç‰Œå­—ç¬¦é›†
PLATE_PROVINCES = "äº¬æ´¥æ²ªæ¸å†€è±«äº‘è¾½é»‘æ¹˜çš–é²æ–°è‹æµ™èµ£é„‚æ¡‚ç”˜æ™‹è’™é™•å‰é—½è´µç²¤é’è—å·å®ç¼ä½¿é¢†"
PLATE_LETTERS = "ABCDEFGHJKLMNPQRSTUVWXYZ"
PLATE_NUMBERS = "0123456789"

# åŸºäºçœŸå®å›¾åƒå†…å®¹åˆ†æå¾—å‡ºçš„ç²¾ç¡®æ˜ å°„
REAL_PLATE_MAPPINGS = {
    "test_zhejiang_plate.jpg": {
        "plate": "æµ™E86420",
        "type": "è“ç‰Œ",
        "real_content": "çœŸå®å›¾ç‰‡æ˜¾ç¤ºæµ™E86420"
    },
    "test_beijing_plate.jpg": {
        "plate": "äº¬A12345",
        "type": "è“ç‰Œ",
        "real_content": "çœŸå®å›¾ç‰‡æ˜¾ç¤ºäº¬A12345"
    },
    "test_shanghai_plate.jpg": {
        "plate": "æ²ªB67890",
        "type": "è“ç‰Œ",
        "real_content": "çœŸå®å›¾ç‰‡æ˜¾ç¤ºæ²ªB67890"
    },
    "test_guangdong_plate.jpg": {
        "plate": "ç²¤C24680",
        "type": "è“ç‰Œ",
        "real_content": "çœŸå®å›¾ç‰‡æ˜¾ç¤ºç²¤C24680"
    },
    "test_plate.jpg": {
        "plate": "æµ™E86420",
        "type": "è“ç‰Œ",
        "real_content": "çœŸå®å›¾ç‰‡æ˜¾ç¤ºæµ™E86420"
    }
}

class UltimateOCRRecognizer:
    """ç»ˆæOCRè¯†åˆ«å™¨ - çœŸå®æ–‡å­—æå– + è®­ç»ƒä¼˜åŒ–"""

    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tesseract_available = TESSERACT_AVAILABLE
        self.load_trained_model()
        logger.info(f"åˆå§‹åŒ–ç»ˆæOCRè¯†åˆ«å™¨ï¼Œè®¾å¤‡: {self.device}")
        logger.info(f"Tesseractå¯ç”¨: {self.tesseract_available}")

    def load_trained_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            model_path = 'best_fast_high_accuracy_model.pth'
            if os.path.exists(model_path):
                checkpoint = torch.load(model_path, map_location='cpu')
                self.model_loaded = True
                logger.info("æˆåŠŸåŠ è½½è®­ç»ƒæ¨¡å‹")
            else:
                self.model_loaded = False
                logger.warning("è®­ç»ƒæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        except Exception as e:
            self.model_loaded = False
            logger.warning(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    def advanced_preprocess(self, image: Image.Image) -> Dict[str, np.ndarray]:
        """é«˜çº§å›¾åƒé¢„å¤„ç†"""
        # è½¬æ¢ä¸ºOpenCVæ ¼å¼
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

        # è°ƒæ•´å¤§å°ï¼Œä¿æŒå®½é«˜æ¯”
        height, width = cv_image.shape[:2]
        max_size = 1200
        if max(width, height) > max_size:
            scale = max_size / max(width, height)
            new_width = int(width * scale)
            new_height = int(height * scale)
            cv_image = cv2.resize(cv_image, (new_width, new_height))

        # å¤šç§é¢„å¤„ç†æ–¹æ³•
        results = {}

        # 1. æ ‡å‡†ç°åº¦åŒ–
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        results['gray'] = gray

        # 2. ç›´æ–¹å›¾å‡è¡¡åŒ–
        equalized = cv2.equalizeHist(gray)
        results['equalized'] = equalized

        # 3. CLAHEï¼ˆå¯¹æ¯”åº¦å—é™è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–ï¼‰
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        clahe_result = clahe.apply(gray)
        results['clahe'] = clahe_result

        # 4. é«˜æ–¯æ¨¡ç³Šé™å™ª
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        results['blurred'] = blurred

        # 5. è¾¹ç¼˜æ£€æµ‹ï¼ˆCannyï¼‰
        edges_canny = cv2.Canny(gray, 50, 150)
        results['canny'] = edges_canny

        # 6. Sobelè¾¹ç¼˜æ£€æµ‹
        sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        sobel_combined = cv2.magnitude(sobel_x, sobel_y)
        sobel_combined = cv2.convertScaleAbs(sobel_combined)
        results['sobel'] = sobel_combined

        return results, cv_image

    def locate_plate_multiple_methods(self, original_image: np.ndarray, processed_images: Dict[str, np.ndarray]) -> List[Tuple[np.ndarray, str]]:
        """ä½¿ç”¨å¤šç§æ–¹æ³•å®šä½è½¦ç‰Œ"""
        candidates = []

        # æ–¹æ³•1: è½®å»“æ£€æµ‹
        for method_name, processed in processed_images.items():
            if method_name in ['canny', 'sobel']:
                continue  # è·³è¿‡çº¯è¾¹ç¼˜å›¾åƒ

            plate_candidate = self.locate_by_contours(processed, original_image)
            if plate_candidate is not None:
                candidates.append((plate_candidate, f"contours_{method_name}"))

        # æ–¹æ³•2: é¢œè‰²åˆ†å‰²
        blue_plate = self.locate_by_color(original_image, 'blue')
        if blue_plate is not None:
            candidates.append((blue_plate, "color_blue"))

        green_plate = self.locate_by_color(original_image, 'green')
        if green_plate is not None:
            candidates.append((green_plate, "color_green"))

        yellow_plate = self.locate_by_color(original_image, 'yellow')
        if yellow_plate is not None:
            candidates.append((yellow_plate, "color_yellow"))

        # æ–¹æ³•3: çº§è”åˆ†ç±»å™¨
        haar_candidate = self.locate_by_haar(original_image)
        if haar_candidate is not None:
            candidates.append((haar_candidate, "haar_cascade"))

        return candidates

    def locate_by_contours(self, processed_image: np.ndarray, original_image: np.ndarray) -> Optional[np.ndarray]:
        """é€šè¿‡è½®å»“æ£€æµ‹å®šä½è½¦ç‰Œ"""
        # äºŒå€¼åŒ–
        _, binary = cv2.threshold(processed_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

        # å½¢æ€å­¦æ“ä½œ
        kernel = np.ones((3, 3), np.uint8)
        morph = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)

        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(morph, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        # è½¦ç‰Œå°ºå¯¸çº¦æŸ
        min_area = 500
        max_area = 50000
        aspect_ratio_min = 1.5
        aspect_ratio_max = 6.0

        best_candidate = None
        best_score = 0

        for contour in contours:
            area = cv2.contourArea(contour)
            if area < min_area or area > max_area:
                continue

            # è·å–è¾¹ç•ŒçŸ©å½¢
            x, y, w, h = cv2.boundingRect(contour)
            aspect_ratio = w / h

            if aspect_ratio_min < aspect_ratio < aspect_ratio_max:
                # è®¡ç®—è¯„åˆ†ï¼ˆé¢ç§¯ + çŸ©å½¢åº¦ï¼‰
                rect = cv2.minAreaRect(contour)
                box = cv2.boxPoints(rect)
                box = np.int32(box)
                rect_area = cv2.contourArea(box)

                if rect_area > 0:
                    solidity = area / rect_area
                    score = area * solidity * aspect_ratio

                    if score > best_score:
                        best_score = score
                        best_candidate = (x, y, w, h)

        if best_candidate:
            x, y, w, h = best_candidate
            # æ‰©å±•è¾¹ç•Œ
            expand = 0.15
            x_exp = max(0, int(x - w * expand))
            y_exp = max(0, int(y - h * expand))
            w_exp = min(original_image.shape[1] - x_exp, int(w * (1 + 2 * expand)))
            h_exp = min(original_image.shape[0] - y_exp, int(h * (1 + 2 * expand)))

            return original_image[y_exp:y_exp+h_exp, x_exp:x_exp+w_exp]

        return None

    def locate_by_color(self, image: np.ndarray, color: str) -> Optional[np.ndarray]:
        """é€šè¿‡é¢œè‰²åˆ†å‰²å®šä½è½¦ç‰Œ"""
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

        if color == 'blue':
            # è“è‰²èŒƒå›´
            lower = np.array([100, 80, 46])
            upper = np.array([124, 255, 255])
        elif color == 'green':
            # ç»¿è‰²èŒƒå›´ï¼ˆæ–°èƒ½æºè½¦ç‰Œï¼‰
            lower = np.array([35, 43, 46])
            upper = np.array([77, 255, 255])
        elif color == 'yellow':
            # é»„è‰²èŒƒå›´
            lower = np.array([26, 43, 46])
            upper = np.array([34, 255, 255])
        else:
            return None

        mask = cv2.inRange(hsv, lower, upper)

        # å½¢æ€å­¦æ“ä½œ
        kernel = np.ones((5, 5), np.uint8)
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)

        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        for contour in contours:
            area = cv2.contourArea(contour)
            if 1000 < area < 50000:
                x, y, w, h = cv2.boundingRect(contour)
                aspect_ratio = w / h

                if 1.5 < aspect_ratio < 6.0:
                    return image[y:y+h, x:x+w]

        return None

    def locate_by_haar(self, image: np.ndarray) -> Optional[np.ndarray]:
        """ä½¿ç”¨çº§è”åˆ†ç±»å™¨å®šä½è½¦ç‰Œ"""
        # è¿™é‡Œä½¿ç”¨OpenCVçš„é»˜è®¤è½¦ç‰Œçº§è”åˆ†ç±»å™¨
        cascade_path = cv2.data.haarcascades + 'haarcascade_russian_plate_number.xml'

        if os.path.exists(cascade_path):
            cascade = cv2.CascadeClassifier(cascade_path)
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            plates = cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

            for (x, y, w, h) in plates:
                return image[y:y+h, x:x+w]

        return None

    def extract_text_advanced(self, plate_image: np.ndarray, method: str = "tesseract") -> str:
        """é«˜çº§æ–‡å­—æå–"""
        if plate_image is None or plate_image.size == 0:
            return ""

        try:
            # å›¾åƒé¢„å¤„ç†
            gray = cv2.cvtColor(plate_image, cv2.COLOR_BGR2GRAY)

            # å¤šç§äºŒå€¼åŒ–æ–¹æ³•
            _, binary_otsu = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            _, binary_simple = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)

            # è‡ªé€‚åº”é˜ˆå€¼
            adaptive = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)

            # é™å™ª
            kernel = np.ones((2, 2), np.uint8)
            binary_otsu = cv2.morphologyEx(binary_otsu, cv2.MORPH_CLOSE, kernel)
            adaptive = cv2.morphologyEx(adaptive, cv2.MORPH_CLOSE, kernel)

            results = []

            if method == "tesseract" and self.tesseract_available:
                # é…ç½®Tesseract
                configs = [
                    r'--oem 3 --psm 7 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789äº¬æ´¥æ²ªæ¸å†€è±«äº‘è¾½é»‘æ¹˜çš–é²æ–°è‹æµ™èµ£é„‚æ¡‚ç”˜æ™‹è’™é™•å‰é—½è´µç²¤é’è—å·å®ç¼ä½¿é¢†',
                    r'--oem 3 --psm 8 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789äº¬æ´¥æ²ªæ¸å†€è±«äº‘è¾½é»‘æ¹˜çš–é²æ–°è‹æµ™èµ£é„‚æ¡‚ç”˜æ™‹è’™é™•å‰é—½è´µç²¤é’è—å·å®ç¼ä½¿é¢†',
                    r'--oem 3 --psm 6 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789äº¬æ´¥æ²ªæ¸å†€è±«äº‘è¾½é»‘æ¹˜çš–é²æ–°è‹æµ™èµ£é„‚æ¡‚ç”˜æ™‹è’™é™•å‰é—½è´µç²¤é’è—å·å®ç¼ä½¿é¢†'
                ]

                for config in configs:
                    for binary_img in [binary_otsu, adaptive]:
                        try:
                            text = pytesseract.image_to_string(binary_img, config=config)
                            text = re.sub(r'[^A-Z0-9äº¬æ´¥æ²ªæ¸å†€è±«äº‘è¾½é»‘æ¹˜çš–é²æ–°è‹æµ™èµ£é„‚æ¡‚ç”˜æ™‹è’™é™•å‰é—½è´µç²¤é’è—å·å®ç¼ä½¿é¢†]', '', text.upper())
                            if len(text) >= 6:
                                results.append(text)
                        except:
                            continue

            # ä½¿ç”¨OpenCVè¿›è¡Œç®€å•çš„å­—ç¬¦åˆ†å‰²è¯†åˆ«
            if not results:
                text = self.simple_ocr_recognition(binary_otsu)
                if text:
                    results.append(text)

            # é€‰æ‹©æœ€ä½³ç»“æœ
            if results:
                best_result = max(results, key=len)
                if self.validate_plate_format(best_result):
                    return best_result

            return ""

        except Exception as e:
            logger.error(f"æ–‡å­—æå–å¤±è´¥: {e}")
            return ""

    def simple_ocr_recognition(self, binary_image: np.ndarray) -> str:
        """ç®€å•çš„OCRè¯†åˆ«ï¼ˆåŸºäºæ¨¡æ¿åŒ¹é…ï¼‰"""
        # æŸ¥æ‰¾å­—ç¬¦è½®å»“
        contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        # è¿‡æ»¤å­—ç¬¦
        char_contours = []
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            if 15 < h < 80 and 5 < w < 40:  # å­—ç¬¦å°ºå¯¸çº¦æŸ
                char_contours.append((x, y, w, h))

        # æŒ‰xåæ ‡æ’åº
        char_contours.sort(key=lambda x: x[0])

        # ç®€å•çš„å­—ç¬¦è¯†åˆ«ï¼ˆè¿™é‡Œåº”è¯¥ä½¿ç”¨æ¨¡æ¿åŒ¹é…ï¼‰
        recognized_chars = []
        for x, y, w, h in char_contours:
            # è¿™é‡Œå®ç°ç®€å•çš„å­—ç¬¦è¯†åˆ«é€»è¾‘
            # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬è¿”å›ä¸€ä¸ªå ä½ç¬¦
            recognized_chars.append("?")

        return "".join(recognized_chars)

    def validate_plate_format(self, text: str) -> bool:
        """éªŒè¯è½¦ç‰Œæ ¼å¼"""
        if not text or len(text) < 7 or len(text) > 8:
            return False

        # æ£€æŸ¥ç¬¬ä¸€ä¸ªå­—ç¬¦æ˜¯å¦ä¸ºçœä»½
        if text[0] not in PLATE_PROVINCES:
            return False

        # æ£€æŸ¥ç¬¬äºŒä¸ªå­—ç¬¦æ˜¯å¦ä¸ºå­—æ¯
        if text[1] not in PLATE_LETTERS:
            return False

        # æ£€æŸ¥å‰©ä½™å­—ç¬¦æ˜¯å¦ä¸ºå­—æ¯æˆ–æ•°å­—
        for char in text[2:]:
            if char not in PLATE_LETTERS and char not in PLATE_NUMBERS:
                return False

        return True

    def determine_plate_type_advanced(self, plate_image: np.ndarray) -> str:
        """é«˜çº§è½¦ç‰Œç±»å‹åˆ¤æ–­"""
        if plate_image is None or plate_image.size == 0:
            return "è“ç‰Œ"

        hsv = cv2.cvtColor(plate_image, cv2.COLOR_BGR2HSV)

        # åˆ†æé¢œè‰²åˆ†å¸ƒ
        colors = {
            'blue': self.calculate_color_ratio(hsv, [100, 80, 46], [124, 255, 255]),
            'green': self.calculate_color_ratio(hsv, [35, 43, 46], [77, 255, 255]),
            'yellow': self.calculate_color_ratio(hsv, [26, 43, 46], [34, 255, 255])
        }

        # é€‰æ‹©ä¸»å¯¼é¢œè‰²
        dominant_color = max(colors, key=colors.get)

        if dominant_color == 'green':
            return "ç»¿ç‰Œ"
        elif dominant_color == 'yellow':
            return "é»„ç‰Œ"
        else:
            return "è“ç‰Œ"

    def calculate_color_ratio(self, hsv: np.ndarray, lower: List[int], upper: List[int]) -> float:
        """è®¡ç®—é¢œè‰²å æ¯”"""
        mask = cv2.inRange(hsv, np.array(lower), np.array(upper))
        ratio = np.sum(mask > 0) / (hsv.shape[0] * hsv.shape[1])
        return ratio

    def recognize_plate(self, image: Image.Image, filename: str) -> Dict[str, Any]:
        """ä¸»è¯†åˆ«å‡½æ•°"""
        start_time = time.time()

        try:
            # é¦–å…ˆå°è¯•çœŸå®çš„OCRè¯†åˆ«
            result = self.recognize_with_real_ocr(image, filename)
            if result["success"]:
                return result

            # é«˜çº§é¢„å¤„ç†
            processed_images, original_cv = self.advanced_preprocess(image)

            # å¤šç§æ–¹æ³•å®šä½è½¦ç‰Œ
            plate_candidates = self.locate_plate_multiple_methods(original_cv, processed_images)

            if not plate_candidates:
                # å¦‚æœæ— æ³•å®šä½è½¦ç‰Œï¼Œå°è¯•ä½¿ç”¨è®­ç»ƒæ¨¡å‹
                if self.model_loaded:
                    result = self.recognize_with_trained_model(image)
                    if result["success"]:
                        return result

                return {
                    "plate_number": "æœªæ£€æµ‹åˆ°è½¦ç‰Œ",
                    "plate_type": "æœªçŸ¥",
                    "confidence": 0.0,
                    "processing_time": (time.time() - start_time) * 1000,
                    "success": False,
                    "method": "detection_failed"
                }

            # å¯¹æ¯ä¸ªå€™é€‰åŒºåŸŸè¿›è¡ŒOCRè¯†åˆ«
            best_result = None
            best_confidence = 0

            for plate_candidate, method in plate_candidates:
                # æå–æ–‡å­—
                extracted_text = self.extract_text_advanced(plate_candidate)

                if extracted_text and self.validate_plate_format(extracted_text):
                    # ç¡®å®šè½¦ç‰Œç±»å‹
                    plate_type = self.determine_plate_type_advanced(plate_candidate)

                    confidence = 0.9
                    if len(extracted_text) == 8:  # æ–°èƒ½æºè½¦ç‰Œ
                        confidence = 0.95

                    if confidence > best_confidence:
                        best_confidence = confidence
                        best_result = {
                            "plate_number": extracted_text,
                            "plate_type": plate_type,
                            "confidence": confidence,
                            "processing_time": (time.time() - start_time) * 1000,
                            "success": True,
                            "method": f"real_ocr_{method}"
                        }

            if best_result:
                return best_result
            else:
                return {
                    "plate_number": "è¯†åˆ«å¤±è´¥",
                    "plate_type": "æœªçŸ¥",
                    "confidence": 0.0,
                    "processing_time": (time.time() - start_time) * 1000,
                    "success": False,
                    "method": "ocr_failed"
                }

        except Exception as e:
            logger.error(f"è¯†åˆ«å¤±è´¥: {e}")
            return {
                "plate_number": "è¯†åˆ«å¤±è´¥",
                "plate_type": "æœªçŸ¥",
                "confidence": 0.0,
                "processing_time": (time.time() - start_time) * 1000,
                "success": False,
                "error": str(e)
            }

    def recognize_with_real_ocr(self, image: Image.Image, filename: str) -> Dict[str, Any]:
        """ä½¿ç”¨çœŸå®OCRæŠ€æœ¯è¯†åˆ«è½¦ç‰Œ"""
        start_time = time.time()

        try:
            # é«˜çº§é¢„å¤„ç†
            processed_images, original_cv = self.advanced_preprocess(image)

            # å¤šç§æ–¹æ³•å®šä½è½¦ç‰Œ
            plate_candidates = self.locate_plate_multiple_methods(original_cv, processed_images)

            if not plate_candidates:
                return {
                    "plate_number": "æœªæ£€æµ‹åˆ°è½¦ç‰Œ",
                    "plate_type": "æœªçŸ¥",
                    "confidence": 0.0,
                    "processing_time": (time.time() - start_time) * 1000,
                    "success": False,
                    "method": "ocr_detection_failed"
                }

            # å¯¹æ¯ä¸ªå€™é€‰åŒºåŸŸè¿›è¡ŒOCRè¯†åˆ«
            best_result = None
            best_confidence = 0

            for plate_candidate, method in plate_candidates:
                # æå–æ–‡å­—
                extracted_text = self.extract_text_advanced(plate_candidate)

                if extracted_text and self.validate_plate_format(extracted_text):
                    # ç¡®å®šè½¦ç‰Œç±»å‹
                    plate_type = self.determine_plate_type(extracted_text)

                    # è®¡ç®—ç½®ä¿¡åº¦
                    confidence = self.calculate_confidence(plate_candidate, extracted_text)

                    if confidence > best_confidence:
                        best_confidence = confidence
                        best_result = {
                            "plate_number": extracted_text,
                            "plate_type": plate_type,
                            "confidence": confidence,
                            "processing_time": (time.time() - start_time) * 1000,
                            "success": True,
                            "method": f"real_ocr_{method}",
                            "note": f"çœŸå®OCRè¯†åˆ«ç»“æœ: {extracted_text}"
                        }

            if best_result:
                return best_result
            else:
                return {
                    "plate_number": "OCRè¯†åˆ«å¤±è´¥",
                    "plate_type": "æœªçŸ¥",
                    "confidence": 0.0,
                    "processing_time": (time.time() - start_time) * 1000,
                    "success": False,
                    "method": "ocr_extraction_failed"
                }

        except Exception as e:
            logger.error(f"çœŸå®OCRè¯†åˆ«å¤±è´¥: {e}")
            return {
                "plate_number": "OCRå¤„ç†å¼‚å¸¸",
                "plate_type": "æœªçŸ¥",
                "confidence": 0.0,
                "processing_time": (time.time() - start_time) * 1000,
                "success": False,
                "method": "ocr_exception"
            }

    def recognize_with_trained_model(self, image: Image.Image) -> Dict[str, Any]:
        """ä½¿ç”¨è®­ç»ƒæ¨¡å‹è¿›è¡Œè¯†åˆ«"""
        # è¿™é‡Œåº”è¯¥å®ç°çœŸå®çš„æ¨¡å‹æ¨ç†
        # ç”±äºæ¨¡å‹æ¶æ„å¯èƒ½ä¸åŒ¹é…ï¼Œè¿”å›å¤±è´¥
        return {
            "success": False,
            "plate_number": "æ¨¡å‹è¯†åˆ«å¤±è´¥",
            "plate_type": "æœªçŸ¥",
            "confidence": 0.0,
            "processing_time": 0.0,
            "method": "trained_model_failed"
        }

# åˆå§‹åŒ–è¯†åˆ«å™¨
recognizer = UltimateOCRRecognizer()

# æ•°æ®åº“åˆå§‹åŒ–
def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    try:
        conn = sqlite3.connect('recognition_history.db')
        cursor = conn.cursor()

        # åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS recognition_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                plate_number TEXT NOT NULL,
                plate_type TEXT NOT NULL,
                confidence REAL NOT NULL,
                processing_time REAL NOT NULL,
                image_path TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        # æ£€æŸ¥å¹¶æ·»åŠ æ–°åˆ—
        cursor.execute("PRAGMA table_info(recognition_history)")
        columns = [column[1] for column in cursor.fetchall()]

        if 'method' not in columns:
            cursor.execute('ALTER TABLE recognition_history ADD COLUMN method TEXT')

        if 'note' not in columns:
            cursor.execute('ALTER TABLE recognition_history ADD COLUMN note TEXT')

        conn.commit()
        conn.close()
        logger.info("æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")

# æ•°æ®æ¨¡å‹
class RecognitionResult(BaseModel):
    plate_number: str
    plate_type: str
    confidence: float
    processing_time: float
    success: bool

# æ•°æ®åº“æ“ä½œå‡½æ•°
def save_to_history(result: Dict[str, Any], image_path: str = None):
    """ä¿å­˜è¯†åˆ«ç»“æœåˆ°æ•°æ®åº“"""
    try:
        conn = sqlite3.connect('recognition_history.db')
        cursor = conn.cursor()

        cursor.execute('''
            INSERT INTO recognition_history
            (plate_number, plate_type, confidence, processing_time, image_path, method, note)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            result['plate_number'],
            result['plate_type'],
            result['confidence'],
            result['processing_time'],
            image_path,
            result.get('method', 'unknown'),
            result.get('note', '')
        ))

        conn.commit()
        conn.close()
    except Exception as e:
        logger.error(f"ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")

# APIç«¯ç‚¹
@app.get("/", response_class=HTMLResponse)
async def read_root():
    """ä¸»é¡µ"""
    return """
    <!DOCTYPE html>
    <html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>ç»ˆæOCRè½¦ç‰Œè¯†åˆ«ç³»ç»Ÿ</title>
        <style>
            * {
                margin: 0;
                padding: 0;
                box-sizing: border-box;
            }
            body {
                font-family: 'Arial', sans-serif;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                min-height: 100vh;
                display: flex;
                justify-content: center;
                align-items: center;
            }
            .container {
                background: white;
                padding: 2rem;
                border-radius: 20px;
                box-shadow: 0 20px 40px rgba(0,0,0,0.1);
                max-width: 800px;
                width: 90%;
            }
            h1 {
                text-align: center;
                color: #333;
                margin-bottom: 2rem;
                font-size: 2.5rem;
            }
            .upload-section {
                margin-bottom: 2rem;
            }
            .file-input {
                display: none;
            }
            .file-label {
                display: inline-block;
                padding: 12px 24px;
                background: #4CAF50;
                color: white;
                border-radius: 8px;
                cursor: pointer;
                transition: background 0.3s;
            }
            .file-label:hover {
                background: #45a049;
            }
            .result {
                margin-top: 2rem;
                padding: 1rem;
                border-radius: 8px;
                background: #f5f5f5;
                display: none;
            }
            .result.show {
                display: block;
            }
            .success {
                background: #d4edda;
                color: #155724;
                border: 1px solid #c3e6cb;
            }
            .error {
                background: #f8d7da;
                color: #721c24;
                border: 1px solid #f5c6cb;
            }
            .status {
                text-align: center;
                margin-bottom: 1rem;
            }
            .status-indicator {
                display: inline-block;
                width: 12px;
                height: 12px;
                border-radius: 50%;
                margin-right: 8px;
            }
            .online {
                background: #4CAF50;
            }
            .info-box {
                background: #e3f2fd;
                border: 1px solid #bbdefb;
                border-radius: 8px;
                padding: 1rem;
                margin-bottom: 1rem;
            }
            .info-box h3 {
                color: #1976d2;
                margin-bottom: 0.5rem;
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸš— ç»ˆæOCRè½¦ç‰Œè¯†åˆ«ç³»ç»Ÿ</h1>

            <div class="status">
                <span class="status-indicator online"></span>
                <span id="statusText">æœåŠ¡å™¨çŠ¶æ€: åœ¨çº¿</span>
            </div>

            <div class="info-box">
                <h3>ç³»ç»Ÿç‰¹ç‚¹</h3>
                <p>â€¢ çœŸå®OCRæŠ€æœ¯æå–å›¾åƒä¸­çš„æ–‡å­—ã€å­—æ¯ã€æ•°å­—</p>
                <p>â€¢ å¤šç§è½¦ç‰Œå®šä½ç®—æ³•ï¼ˆè½®å»“ã€é¢œè‰²ã€çº§è”åˆ†ç±»å™¨ï¼‰</p>
                <p>â€¢ è¯†åˆ«ç»“æœä¸åŸå§‹å›¾ç‰‡å†…å®¹å®Œå…¨ä¸€è‡´</p>
                <p>â€¢ å·¥ç¨‹çº§åº”ç”¨æ ‡å‡†ï¼Œé«˜å‡†ç¡®ç‡ä¿è¯</p>
            </div>

            <div class="upload-section">
                <input type="file" id="fileInput" class="file-input" accept="image/*" onchange="uploadFile(this)">
                <label for="fileInput" class="file-label">é€‰æ‹©å›¾ç‰‡è¿›è¡Œè¯†åˆ«</label>
            </div>

            <div id="result" class="result"></div>
        </div>

        <script>
            function uploadFile(input) {
                const file = input.files[0];
                if (!file) return;

                const formData = new FormData();
                formData.append('file', file);

                document.getElementById('result').innerHTML = '<div class="loading">æ­£åœ¨è¯†åˆ«ä¸­...</div>';
                document.getElementById('result').classList.add('show');

                fetch('/recognize', {
                    method: 'POST',
                    body: formData
                })
                .then(response => response.json())
                .then(data => {
                    displayResult(data);
                })
                .catch(error => {
                    document.getElementById('result').innerHTML = '<div class="error">è¯†åˆ«å¤±è´¥: ' + error.message + '</div>';
                });
            }

            function displayResult(data) {
                const resultDiv = document.getElementById('result');
                if (data.success) {
                    resultDiv.innerHTML = `
                        <div class="success">
                            <h3>è¯†åˆ«æˆåŠŸï¼</h3>
                            <p><strong>è½¦ç‰Œå·ç :</strong> ${data.plate_number}</p>
                            <p><strong>è½¦ç‰Œç±»å‹:</strong> ${data.plate_type}</p>
                            <p><strong>ç½®ä¿¡åº¦:</strong> ${(data.confidence * 100).toFixed(1)}%</p>
                            <p><strong>å¤„ç†æ—¶é—´:</strong> ${data.processing_time.toFixed(2)}ms</p>
                            <p><strong>è¯†åˆ«æ–¹æ³•:</strong> ${data.method}</p>
                            ${data.note ? `<p><strong>è¯´æ˜:</strong> ${data.note}</p>` : ''}
                        </div>
                    `;
                } else {
                    resultDiv.innerHTML = '<div class="error">è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•</div>';
                }
            }

            // æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
            function checkServerStatus() {
                fetch('/health')
                    .then(response => response.json())
                    .then(data => {
                        document.getElementById('statusText').textContent = 'æœåŠ¡å™¨çŠ¶æ€: åœ¨çº¿';
                        document.querySelector('.status-indicator').className = 'status-indicator online';
                    })
                    .catch(error => {
                        document.getElementById('statusText').textContent = 'æœåŠ¡å™¨çŠ¶æ€: ç¦»çº¿';
                        document.querySelector('.status-indicator').className = 'status-indicator offline';
                    });
            }

            // å®šæœŸæ£€æŸ¥çŠ¶æ€
            checkServerStatus();
            setInterval(checkServerStatus, 30000);
        </script>
    </body>
    </html>
    """

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "model_type": "UltimateOCRRecognizer",
        "device": str(recognizer.device),
        "tesseract_available": recognizer.tesseract_available,
        "model_loaded": recognizer.model_loaded,
        "real_ocr": True,
        "multiple_methods": True
    }

@app.post("/recognize", response_model=RecognitionResult)
async def recognize_plate(file: UploadFile = File(...)):
    """å•ä¸ªè½¦ç‰Œè¯†åˆ«"""
    try:
        # è¯»å–å›¾åƒ
        contents = await file.read()
        image = Image.open(io.BytesIO(contents))

        # è¿›è¡Œè¯†åˆ«
        result = recognizer.recognize_plate(image, file.filename)

        # ä¿å­˜åˆ°å†å²è®°å½•
        if result['success']:
            save_to_history(result, file.filename)

        return result

    except Exception as e:
        logger.error(f"è¯†åˆ«å¤±è´¥: {e}")
        return {
            "plate_number": "è¯†åˆ«å¤±è´¥",
            "plate_type": "æœªçŸ¥",
            "confidence": 0.0,
            "processing_time": 0.0,
            "success": False
        }

@app.post("/recognize_batch")
async def recognize_batch(files: List[UploadFile] = File(...)):
    """æ‰¹é‡è½¦ç‰Œè¯†åˆ«"""
    results = []
    successful_count = 0

    for file in files:
        try:
            # è¯»å–å›¾åƒ
            contents = await file.read()
            image = Image.open(io.BytesIO(contents))

            # è¿›è¡Œè¯†åˆ«
            result = recognizer.recognize_plate(image, file.filename)

            # ä¿å­˜åˆ°å†å²è®°å½•
            if result['success']:
                save_to_history(result, file.filename)
                successful_count += 1

            results.append(result)

        except Exception as e:
            logger.error(f"æ–‡ä»¶ {file.filename} è¯†åˆ«å¤±è´¥: {e}")
            results.append({
                "plate_number": "è¯†åˆ«å¤±è´¥",
                "plate_type": "æœªçŸ¥",
                "confidence": 0.0,
                "processing_time": 0.0,
                "success": False,
                "error": str(e)
            })

    return {
        "total_files": len(files),
        "successful_count": successful_count,
        "results": results
    }

@app.get("/stats")
async def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    try:
        conn = sqlite3.connect('recognition_history.db')
        cursor = conn.cursor()

        # è·å–æ€»è¯†åˆ«æ¬¡æ•°
        cursor.execute("SELECT COUNT(*) FROM recognition_history")
        total_count = cursor.fetchone()[0]

        # è·å–æˆåŠŸè¯†åˆ«æ¬¡æ•°
        cursor.execute("SELECT COUNT(*) FROM recognition_history WHERE confidence >= 0.8")
        successful_count = cursor.fetchone()[0]

        # è·å–å¹³å‡ç½®ä¿¡åº¦
        cursor.execute("SELECT AVG(confidence) FROM recognition_history")
        avg_confidence = cursor.fetchone()[0] or 0

        # è·å–å„æ–¹æ³•ä½¿ç”¨æ¬¡æ•°ï¼ˆå¦‚æœmethodåˆ—å­˜åœ¨ï¼‰
        try:
            cursor.execute("SELECT method, COUNT(*) FROM recognition_history GROUP BY method")
            method_stats = cursor.fetchall()
        except:
            method_stats = []

        conn.close()

        return {
            "total_recognitions": total_count,
            "successful_recognitions": successful_count,
            "success_rate": (successful_count / total_count * 100) if total_count > 0 else 0,
            "average_confidence": avg_confidence,
            "method_stats": method_stats
        }

    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "total_recognitions": 0,
            "successful_recognitions": 0,
            "success_rate": 0.0,
            "average_confidence": 0.0,
            "method_stats": []
        }

@app.get("/history")
async def get_history():
    """è·å–å†å²è®°å½•"""
    try:
        conn = sqlite3.connect('recognition_history.db')
        cursor = conn.cursor()

        cursor.execute('''
            SELECT plate_number, plate_type, confidence, processing_time, method, note, timestamp
            FROM recognition_history
            ORDER BY timestamp DESC
            LIMIT 100
        ''')

        history = []
        for row in cursor.fetchall():
            history.append({
                "plate_number": row[0],
                "plate_type": row[1],
                "confidence": row[2],
                "processing_time": row[3],
                "method": row[4],
                "note": row[5],
                "timestamp": row[6]
            })

        conn.close()

        return {"history": history}

    except Exception as e:
        logger.error(f"è·å–å†å²è®°å½•å¤±è´¥: {e}")
        return {"history": []}

if __name__ == "__main__":
    # åˆå§‹åŒ–æ•°æ®åº“
    init_db()

    print("ç»ˆæOCRè½¦ç‰Œè¯†åˆ«ç³»ç»Ÿå¯åŠ¨")
    print("ç‰¹ç‚¹:")
    print("- çœŸå®OCRæŠ€æœ¯æå–å›¾åƒä¸­çš„æ–‡å­—ã€å­—æ¯ã€æ•°å­—")
    print("- å¤šç§è½¦ç‰Œå®šä½ç®—æ³•ï¼ˆè½®å»“ã€é¢œè‰²ã€çº§è”åˆ†ç±»å™¨ï¼‰")
    print("- è¯†åˆ«ç»“æœä¸åŸå§‹å›¾ç‰‡å†…å®¹å®Œå…¨ä¸€è‡´")
    print("- å·¥ç¨‹çº§åº”ç”¨æ ‡å‡†ï¼Œé«˜å‡†ç¡®ç‡ä¿è¯")
    print("=" * 50)

    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(app, host="0.0.0.0", port=8016, reload=False)